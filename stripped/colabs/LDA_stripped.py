# -*- coding: utf-8 -*-
"""LDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VRdBLO9bnE6tdbfPo9Pv77tPwmUac0Iv

# Import i instalacja niezbędnych bibliotek
"""

!pip install datashader
!pip install holoviews hvplot colorcet
!pip install geoviews

# import doinstalowanych pakietów
import datashader as ds
import colorcet as cc
import holoviews as hv
import geoviews as gv
import geoviews.tile_sources as gts
from holoviews import opts
from bokeh.plotting import show, output_notebook

from typing import List

import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

import sklearn
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from imblearn.over_sampling import SMOTE

import os
from google.colab import drive
drive.mount("/content/drive")

!git clone https://github.com/PiotrMaciejKowalski/BigData2024Project.git

!chmod 755 /content/BigData2024Project/src/setup.sh
!/content/BigData2024Project/src/setup.sh

import sys
sys.path.append('/content/BigData2024Project/src')

from start_spark import initialize_spark
initialize_spark()

from pyspark.sql import SparkSession
from big_mess.loaders import preprocessed_loader, load_single_month, save_to_csv, load_anotated
from big_mess.agg_classification_eval import plot_map

"""Funkcje służące przedstawieniu metryk związanych z klasyfikacją oraz zobrazowaniu macierzy pomyłek:"""

def show_metrics(model: sklearn.base.BaseEstimator, X: pd.DataFrame, y: pd.DataFrame) -> None:
    """
    Funkcja zwracająca cztery metryki oceniające klasyfikator.

    :param model: model klasyfikujący, który będzie oceniany
    :param X: zmienne objaśniające
    :param y: zmienna objaśniana, której predykcji chcemy dokonać
    """
    y_pred = model.predict(X)

    accuracy = accuracy_score(y, y_pred)
    print(f"Dokładność (accuracy): {accuracy*100:.2f}%")
    precision = precision_score(y, y_pred)
    print(f"Precyzja (precision): {precision*100:.2f}%")
    recall = recall_score(y, y_pred)
    print(f"Czułość (recall): {recall*100:.2f}%")
    f1 = f1_score(y, y_pred)
    print(f"F1-score: {f1*100:.2f}%")

def summary_model(model: sklearn.base.BaseEstimator, X: pd.DataFrame, y: pd.DataFrame, labels_names: List) -> None:
  y_pred = model.predict(X)
  y_real= y
  cf_matrix = confusion_matrix(y_real, y_pred)
  group_counts = ["{0:0.0f}".format(value) for value in cf_matrix.flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]
  labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(len(labels_names),len(labels_names))
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Reds',xticklabels=labels_names,yticklabels=labels_names)
  plt.xlabel('Predykcja')
  plt.ylabel('Rzeczywistość')
  plt.show()

spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

"""# Przygotowanie danych

Wczytanie początkowych oraz zaanotowanych danych z lipca 2023 (jako że on wypadał najlepiej w pozostałych klasyfikacjach):
"""

# zbiór zawierający pomiary z lipca 2023

# NASA_202307 = load_single_month(spark, year=2023, month=7)
# save_to_csv(NASA_202307, '/content/drive/MyDrive/BigMess/NASA/months_preprocessed/LDA/NASA_202307.csv')
NASA_202307 = preprocessed_loader(spark,'/content/drive/MyDrive/BigMess/NASA/months_preprocessed/LDA/NASA_202307.csv').toPandas()

# zaanotowany zbiór z danymi z lipca 2023
nasa_anotated_202307 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202307.csv')
df_202307 = nasa_anotated_202307.toPandas()

df_202307.head()

"""Anotowany dataset przedstawiony na mapie:"""

output_notebook()
show(plot_map(df=df_202307, parameter_name='pustynia',
              colormap=dict(zip(['1', '0'], ['yellow', 'green'])),
              title='Pustynie (1) i niepustynie (0) - zbiór anotowany',
              point_size=6, alpha=0.7))

"""# Klasyfikacja pustynia/niepustynia"""

X_202307 = df_202307.loc[:,'Rainf':'SoilM_100_200cm']
y_202307 = df_202307['pustynia']

"""Podział na zbiór treningowy i testowy:"""

X_202307_train, X_202307_test, y_202307_train, y_202307_test = train_test_split(X_202307, y_202307, test_size=0.2, random_state=0)

"""## Podstawowy model LDA z domyślnymi parametrami:"""

LDA_202307_default = LinearDiscriminantAnalysis()
LDA_202307_default.fit(X_202307_train, y_202307_train)

"""Zastosujmy model na pełnych danych z lipca 2023 i zwizualizujmy go na mapie:"""

LDA_202307_default_pred_FULL = NASA_202307.copy()
LDA_202307_default_pred_FULL['pustynia'] = LDA_202307_default.predict(LDA_202307_default_pred_FULL.loc[:,'Rainf':'SoilM_100_200cm'])

output_notebook()
show(plot_map(df=LDA_202307_default_pred_FULL, parameter_name='pustynia',
              colormap=dict(zip(['1', '0'], ['yellow', 'green'])),
              title='Pustynie (1) i niepustynie (0) - LDA (lipiec 2023, domyślne parametry)',
              point_size=6, alpha=0.7))

"""Porównując tę mapę z mapą dla zbioru anotowanego, można zauważyć pewne odchyłki w klasyfikacji.

#### Ocena modelu na zbiorze treningowym
"""

summary_model(LDA_202307_default, X_202307_train, y_202307_train, ['0','1'])

show_metrics(LDA_202307_default, X_202307_train, y_202307_train)

"""### Ocena na zbiorze testowym"""

summary_model(LDA_202307_default, X_202307_test, y_202307_test, ['0', '1'])

show_metrics(LDA_202307_default, X_202307_test, y_202307_test)

"""### Obserwacje

Model zarówno na zbiorze treningowym, jak i testowym wypada całkiem dobrze. Otrzymujemy na zbiorze testowym accuracy wynoszące 93.31%. Precyzja w przypadku zbioru testowego odrobinę się zmniejszyła (o ok. 0.14 p. p.), natomiast wartości pozostałych metryk są wyższe niż dla zbioru treningowego.

# GridSearch

Spróbujmy wykorzystując GridSearch'a znaleźć parametry, dla których model powinien dawać najlepsze wyniki.
"""

param_grid = {
    'solver': ['svd', 'lsqr', 'eigen'],
    'shrinkage': ['auto', None],
    'store_covariance': [True, False],
    'tol': [1e-4, 1e-5, 1e-6]
}

lda = LinearDiscriminantAnalysis()

grid_search = GridSearchCV(lda, param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_202307_train, y_202307_train)

print("Najlepsze parametry:", grid_search.best_params_)

gridsearch_LDA_model = grid_search.best_estimator_
gridsearch_LDA_model.fit(X_202307_train, y_202307_train)

"""Zastosujmy model na pełnych danych:"""

LDA_202307_gridsearch_model_pred_FULL = NASA_202307.copy()
LDA_202307_gridsearch_model_pred_FULL['pustynia'] = gridsearch_LDA_model.predict(LDA_202307_gridsearch_model_pred_FULL.loc[:,'Rainf':'SoilM_100_200cm'])

output_notebook()
show(plot_map(df=LDA_202307_gridsearch_model_pred_FULL, parameter_name='pustynia',
              colormap=dict(zip(['1', '0'], ['yellow', 'green'])),
              title='Pustynie (1) i niepustynie (0) - LDA (lipiec 2023, parametry znalezione przez GridSearch)',
              point_size=6, alpha=0.7))

"""### Ocena modelu na zbiorze treningowym"""

summary_model(gridsearch_LDA_model, X_202307_train, y_202307_train, ['0','1'])

show_metrics(gridsearch_LDA_model, X_202307_train, y_202307_train)

"""### Ocena modelu na zbiorze testowym"""

summary_model(gridsearch_LDA_model, X_202307_test, y_202307_test, ['0', '1'])

show_metrics(gridsearch_LDA_model, X_202307_test, y_202307_test)

"""### Obserwacje

Dla obu zbiorów otrzymaliśmy lepsze wyniki niż w przypadku modelu z domyślnymi parametrami. Jedynie czułość pozostała niezmieniona. Ponadto, teraz wartość każdej z metryk dla zbioru testowego jest wyższa niż dla zbioru treningowego. Accuracy na zbiorze testowym wyniosło 93.72%.

## Zbalansowanie zbioru danych

Spróbujmy jeszcze zbalansować klasy w zbiorze danych.
"""

X_202307_resampled, y_202307_resampled = SMOTE().fit_resample(X_202307_train, y_202307_train)

grid_search_resampled = GridSearchCV(lda, param_grid, cv=5, scoring='accuracy')
grid_search_resampled.fit(X_202307_resampled, y_202307_resampled)

print("Najlepsze parametry:", grid_search_resampled.best_params_)

"""Tym razem zmienił się solver, a shrinkage z 'auto' zmieniło się na 'None'.

### Ocena modelu na zbiorze treningowym
"""

gridsearch_LDA_model_resampled = grid_search_resampled.best_estimator_
gridsearch_LDA_model_resampled.fit(X_202307_resampled, y_202307_resampled)

summary_model(gridsearch_LDA_model_resampled, X_202307_resampled, y_202307_resampled, ['0','1'])

show_metrics(gridsearch_LDA_model_resampled, X_202307_resampled, y_202307_resampled)

"""### Ocena modelu na zbiorze testowym"""

summary_model(gridsearch_LDA_model_resampled, X_202307_test, y_202307_test, ['0', '1'])

show_metrics(gridsearch_LDA_model_resampled, X_202307_test, y_202307_test)

"""### Obserwacje

Niestety tym razem, mimo bardzo obiecujących wyników dla zbioru treningowego, dla zbioru testowego znacząco zmniejszyła się precyzja (o 33 p. p. względem zbioru treningowego, o 17 p. p. w porównaniu do modelu niezbalansowanego). Jedynie czułość jest lepsza o ok. 20 p. p. niż we wcześniejszych przypadkach.

## Końcowy wniosek

W przypadku modelu LDA, jak również części innych stworzonych dla danych NASA modeli, zbalansowanie zbioru danych nie poprawia końcowych wyników. Najlepszy z modeli LDA osiągnął na zbiorze testowym accuracy równe 93.72%, co jest bardzo dobrym wynikiem, jednakże niższym niż w przypadku najlepszego z powstałych w czasie trwania projektu modeli.
"""