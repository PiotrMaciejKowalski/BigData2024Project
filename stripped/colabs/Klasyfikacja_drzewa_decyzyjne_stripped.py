# -*- coding: utf-8 -*-
"""Klasyfikacja_drzewa_decyzyjne.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mhHD8R4Vx6YKmAlTJ1fYQW0aancBLpgz

# Klasyfikacja z wykorzystaniem drzew decyzyjnych

## Wczytywanie danych w sparku

Utworzenie środowiska pyspark do obliczeń:
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/PiotrMaciejKowalski/BigData2024Project.git
# %cd BigData2024Project
!git checkout Klasyfikacja_drzewa_12m
# %cd ..

!chmod 755 /content/BigData2024Project/src/setup.sh
!/content/BigData2024Project/src/setup.sh

import sys
sys.path.append('/content/BigData2024Project/src')

from start_spark import initialize_spark
initialize_spark()

import pandas as pd
from pyspark.sql import SparkSession

from big_mess.loaders import default_loader, load_single_month, load_anotated, save_to_csv, preprocessed_loader

spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

"""## Budowa modeli

**Cel**:

Celem jest zbudowanie modeli do klasyfikacji czy wskazany punkt lokalizacyjny ze zbioru danych NASA jest: pustynia, stepem lub innym obszarem. Zbudowane zostana trzy rodzaje modeli:

* Detekcja pustynia - step - inne
* Detekcja pustynia - niepustynia
* Detekcja step - niestep

Dla kazdego z miesiecy powstanie jeden model kazdego rodzaju.

**Proba danych**:

Dane wykorzystane do modelowania zostały stworzone po przez połączenie dwoch zbiorów danych:

* 500 lokalizacji *lon* i *lat* z określoną flagą 0, 1 w kolumnach *pustynia* lub *step* (reczna adnotacja)
* danych NASA w podziale na miesiące. Od *pazdziernika 2022* do  *wrzesnia 2023*

**Metoda**:

Do modelowania uzyto metody drzew decyzyjnych.

### Import bibliotek
"""

!pip install datashader
!pip install holoviews hvplot colorcet
!pip install geoviews

from typing import List
import pandas as pd
import pickle
import numpy as np
import matplotlib.colors as mpl_colors
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_classif
import datashader as ds
import datashader.transfer_functions as tf
import colorcet as cc
import holoviews as hv
from holoviews.operation.datashader import datashade
import geoviews as gv
import geoviews.tile_sources as gts
from holoviews import opts
from IPython.display import IFrame
from bokeh.plotting import show, output_notebook

"""### Przygotowanie danych"""

#nasa_anotated_202210 = load_anotated(spark,year=2022,month = 10)
#nasa_anotated_202211 = load_anotated(spark,year=2022,month = 11)
#nasa_anotated_202212 = load_anotated(spark,year=2022,month = 12)
#nasa_anotated_202301 = load_anotated(spark,year=2023,month = 1)
#nasa_anotated_202302 = load_anotated(spark,year=2023,month = 2)
#nasa_anotated_202303 = load_anotated(spark,year=2023,month = 3)
#nasa_anotated_202304 = load_anotated(spark,year=2023,month = 4)
#nasa_anotated_202305 = load_anotated(spark,year=2023,month = 5)
#nasa_anotated_202306 = load_anotated(spark,year=2023,month = 6)
#nasa_anotated_202307 = load_anotated(spark,year=2023,month = 7)
#nasa_anotated_202308 = load_anotated(spark,year=2023,month = 8)
#nasa_anotated_202309 = load_anotated(spark,year=2023,month = 9)

#save_to_csv(nasa_anotated_202210, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202210.csv')
#save_to_csv(nasa_anotated_202211, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202211.csv')
#save_to_csv(nasa_anotated_202212, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202212.csv')
#save_to_csv(nasa_anotated_202301, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202301.csv')
#save_to_csv(nasa_anotated_202302, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202302.csv')
#save_to_csv(nasa_anotated_202303, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202303.csv')
#save_to_csv(nasa_anotated_202304, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202304.csv')
#save_to_csv(nasa_anotated_202305, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202305.csv')
#save_to_csv(nasa_anotated_202306, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202306.csv')
#save_to_csv(nasa_anotated_202307, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202307.csv')
#save_to_csv(nasa_anotated_202308, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202308.csv')
#save_to_csv(nasa_anotated_202309, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202309.csv')

nasa_anotated_202210 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202210.csv')
nasa_anotated_202211 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202211.csv')
nasa_anotated_202212 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202212.csv')
nasa_anotated_202301 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202301.csv')
nasa_anotated_202302 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202302.csv')
nasa_anotated_202303 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202303.csv')
nasa_anotated_202304 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202304.csv')
nasa_anotated_202305 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202305.csv')
nasa_anotated_202306 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202306.csv')
nasa_anotated_202307 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202307.csv')
nasa_anotated_202308 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202308.csv')
nasa_anotated_202309 = preprocessed_loader(spark, '/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/Anotated_data_12m/nasa_anotated_202309.csv')

df_202210 = nasa_anotated_202210.toPandas()
df_202211 = nasa_anotated_202211.toPandas()
df_202212 = nasa_anotated_202212.toPandas()
df_202301 = nasa_anotated_202301.toPandas()
df_202302 = nasa_anotated_202302.toPandas()
df_202303 = nasa_anotated_202303.toPandas()
df_202304 = nasa_anotated_202304.toPandas()
df_202305 = nasa_anotated_202305.toPandas()
df_202306 = nasa_anotated_202306.toPandas()
df_202307 = nasa_anotated_202307.toPandas()
df_202308 = nasa_anotated_202308.toPandas()
df_202309 = nasa_anotated_202309.toPandas()

"""Stworzenie listy w ktorej przechowane beda ramki danych dla poszczegolnych miesiecy."""

list_of_df = [df_202301, df_202302, df_202303, df_202304, df_202305, df_202306, df_202307, df_202308, df_202309, df_202210, df_202211, df_202212]

"""Dodanie kolumny *klasa* z nastepujacym oslownikowaniem:

* **1** - pustynia
* **2** - step
* **3** - inne
"""

for df in list_of_df:
  df['klasa'] = np.where(
                        df['pustynia'] == 1, 1, np.where(
                        df['step'] == 1,2,3)
                        )

"""Przykladowa ramka danych."""

df_202307.head()

"""Liczebnosci poszczegolnych klas w probce."""

{1:df_202307['pustynia'].sum(), 2:df_202307['step'].sum() , 3:df_202307['pustynia'].count()- df_202307['pustynia'].sum()- df_202307['step'].sum()}

"""#### Wydzielenie zbiorow danych

Zmienne kandydatki:

* **GVEG** - wskaznik roslinnosci
* **Rainf** - wskaznik opadow deszczu
* **Evap** - wskaznik calkowitej ewapotranspiracji
* **AvgSurfT** - wskaznik sredniej temperatury powierzchni ziemi
* **Albedo** - wskaznik albedo
* **SoilT_40_100cm** - wskaznik temperatury gleby w warstwie o glebokosci od 40 do 100 cm
* **PotEvap** - wskaznik potencjalnej ewapotranspiracji
* **RootMoist** - wilgotnosć gleby w strefie korzeniowej (parowanie, ktore mialoby miejsce, gdyby dostepne bylo wystarczajace zrodlo wody)
* **SoilM_100_200cm** - wilgotnosc gleby w warstwie o glebokosci od 100 do 200 cm
"""

X_202210 = df_202210.loc[:,'Rainf':'SoilM_100_200cm']
X_202211 = df_202211.loc[:,'Rainf':'SoilM_100_200cm']
X_202212 = df_202212.loc[:,'Rainf':'SoilM_100_200cm']
X_202301 = df_202301.loc[:,'Rainf':'SoilM_100_200cm']
X_202302 = df_202302.loc[:,'Rainf':'SoilM_100_200cm']
X_202303 = df_202303.loc[:,'Rainf':'SoilM_100_200cm']
X_202304 = df_202304.loc[:,'Rainf':'SoilM_100_200cm']
X_202305 = df_202305.loc[:,'Rainf':'SoilM_100_200cm']
X_202306 = df_202306.loc[:,'Rainf':'SoilM_100_200cm']
X_202307 = df_202307.loc[:,'Rainf':'SoilM_100_200cm']
X_202308 = df_202308.loc[:,'Rainf':'SoilM_100_200cm']
X_202309 = df_202309.loc[:,'Rainf':'SoilM_100_200cm']

y_m1 = df_202307['klasa']
y_m2 = df_202307['pustynia']
y_m3 = df_202307['step']

"""#### Definiowanie funkcji"""

def information_gain(X: pd.DataFrame, y: pd.DataFrame) -> None:
  importances = mutual_info_classif(X, y)
  feature_info = pd.Series(importances, X.columns).sort_values(ascending=False)
  print(feature_info)

def correlations(X: pd.DataFrame) -> None:
  cor = X.corr()
  sns.heatmap(cor, annot=True,cmap='Reds')

def plot_data_dist(y: pd.DataFrame) -> None:
  dane = pd.Series(y).value_counts().sort_index()
  labels = list(np.sort(pd.unique(y)))
  ypos=np.arange(len(labels))
  plt.xticks(ypos, labels)
  plt.xlabel('Klasa')
  plt.ylabel('Czestosc')
  plt.title('Liczebnosc dla proby')
  plt.bar(ypos,dane)

def summary_model(model, X:pd.DataFrame, y:pd.DataFrame, labels_names: List) -> None:
  y_pred = model.predict(X)
  y_real= y
  cf_matrix = confusion_matrix(y_real, y_pred)
  group_counts = ["{0:0.0f}".format(value) for value in cf_matrix.flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]
  labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(len(labels_names),len(labels_names))
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Reds',xticklabels=labels_names,yticklabels=labels_names)
  plt.xlabel('Predykcja')
  plt.ylabel('Rzeczywistość')
  plt.show()

def print_classification_report(model, X: pd.DataFrame, y: pd.DataFrame) -> None:
  y_predict = model.predict(X)
  print(classification_report(y, y_predict))

def print_accuracy_heatmap(list_of_models: List, list_of_X_test: List, y_test: List) -> None:
  list_accuracy = []
  for i in range(12):
    temp = []
    for j in range(12):
      temp.append(list_of_models[i].score(list_of_X_test[j],  y_test) *100)
    list_accuracy.append(temp)
  miesiace = ['STY', 'LUT', 'MAR', 'KWI', 'MAJ', 'CZE', 'LIP', 'SIE', 'WRZ', 'PAZ', 'LIS', 'GRU']
  df_accuracy =pd.DataFrame(list_accuracy, index=miesiace, columns=miesiace)
  sns.heatmap(df_accuracy, annot=True)
  plt.xlabel('Miesiace - test')
  plt.ylabel('Miesiace - model')
  plt.title("Accuracy")

def print_recall_heatmap(list_of_models: List, list_of_X_test: List, y_test: List) -> None:
  list_recall = []
  for i in range(12):
    temp = []
    for j in range(12):
      y_pred = list_of_models[i].predict(list_of_X_test[j])
      temp.append(recall_score(y_pred,  y_test) *100)
    list_recall.append(temp)
  miesiace = ['STY', 'LUT', 'MAR', 'KWI', 'MAJ', 'CZE', 'LIP', 'SIE', 'WRZ', 'PAZ', 'LIS', 'GRU']
  df_recall =pd.DataFrame(list_recall, index=miesiace, columns=miesiace)
  sns.heatmap(df_recall, annot=True)
  plt.xlabel('Miesiace - test')
  plt.ylabel('Miesiace - model')
  plt.title("Recall")

def print_precision_heatmap(list_of_models: List, list_of_X_test: List, y_test: List) -> None:
  list_precision = []
  for i in range(12):
    temp = []
    for j in range(12):
      y_pred = list_of_models[i].predict(list_of_X_test[j])
      temp.append(precision_score(y_pred,  y_test) *100)
    list_precision.append(temp)
  miesiace = ['STY', 'LUT', 'MAR', 'KWI', 'MAJ', 'CZE', 'LIP', 'SIE', 'WRZ', 'PAZ', 'LIS', 'GRU']
  df_precision =pd.DataFrame(list_precision, index=miesiace, columns=miesiace)
  sns.heatmap(df_precision, annot=True)
  plt.xlabel('Miesiace - test')
  plt.ylabel('Miesiace - model')
  plt.title("Precision")

def print_f1_heatmap(list_of_models: List, list_of_X_test: List, y_test: List) -> None:
  list_f1 = []
  for i in range(12):
    temp = []
    for j in range(12):
      y_pred = list_of_models[i].predict(list_of_X_test[j])
      temp.append(f1_score(y_pred,  y_test) *100)
    list_f1.append(temp)
  miesiace = ['STY', 'LUT', 'MAR', 'KWI', 'MAJ', 'CZE', 'LIP', 'SIE', 'WRZ', 'PAZ', 'LIS', 'GRU']
  df_f1 =pd.DataFrame(list_f1, index=miesiace, columns=miesiace)
  sns.heatmap(df_f1, annot=True)
  plt.xlabel('Miesiace - test')
  plt.ylabel('Miesiace - model')
  plt.title("F1")

from typing import Optional, Tuple
from pandas import DataFrame
from imblearn.over_sampling import RandomOverSampler, SMOTE

class BalanceDataSet():
  '''
  Two techniques for handling imbalanced data.
  '''
  def __init__(
      self,
      X: DataFrame,
      y: DataFrame
      ) -> None:
      self.X = X
      self.y = y
      assert len(self.X)==len(self.y)

  def useOverSampling(
      self,
      randon_seed: Optional[int] = 2023
      ) -> Tuple[DataFrame, DataFrame]:
    oversample = RandomOverSampler( sampling_strategy='auto',
                  random_state=randon_seed)
    return oversample.fit_resample(self.X, self.y)

  def useSMOTE(
      self,
      randon_seed: Optional[int] = 2023
      ) -> Tuple[DataFrame, DataFrame]:
    smote = SMOTE(random_state=randon_seed)
    return smote.fit_resample(self.X, self.y)

def get_colormap(values: list, colors_palette: list, name = 'custom'):
    """
    Funkcja jako argumenty bierze liste wartosci okreslajacych granice przedzialow liczbowych, ktore
    beda okreslac jak dla rozwazanego parametru maja zmieniac się kolory punktow, ktorych lista stanowi
    drugi argument funkcji.
    """
    values = np.sort(np.array(values))
    values = np.interp(values, (values.min(), values.max()), (0, 1))
    cmap = mpl_colors.LinearSegmentedColormap.from_list(name, list(zip(values, colors_palette)))
    return cmap

def plot_map(df: pd.DataFrame, parameter_name: str, colormap: mpl_colors.LinearSegmentedColormap, title: str,
             point_size: int = 8, width: int = 800, height: int = 500, alpha: float = 1,
             bgcolor: str = 'white', colorbar_verbose: bool = True):

    gdf = gv.Points(df, ['lon', 'lat'], [parameter_name]) # obiekt zawierający punkty
    tiles = gts.OSM # wybór mapy tła, w tym wypadku OpenStreetMap

    # łączenie mapy tła z punktami i ustawienie wybranych parametrów wizualizacji
    map_with_points = tiles * gdf.opts(
        title=title,
        color=parameter_name,
        cmap=colormap,
        size=point_size,
        width=width,
        height=height,
        colorbar=colorbar_verbose,
        toolbar='above',
        tools=['hover', 'wheel_zoom', 'reset'],
        alpha=alpha, # przezroczystość
        bgcolor=bgcolor
    )
    return hv.render(map_with_points)

def plot_m2_month(df: pd.DataFrame, month: int) -> None:
  df['Model_2'] = list_of_models_m2[month-1].predict(df.loc[:,'Rainf':'SoilM_100_200cm'])
  colormap_cluster = get_colormap([0, max(df.Model_2.values)], ['darkgreen', 'yellow'])
  plot = plot_map(df=df, parameter_name='Model_2', colormap=colormap_cluster, title="Detekcja pustynia (1) - niepustynia (0)", alpha=1)
  output_notebook()
  show(plot)

"""### Model 1 - detekcja pustynia - step - inne

#### Podzial na zbior treningowy i testowy
"""

X_202210_m1_train, X_202210_m1_test, y_m1_train, y_m1_test = train_test_split(X_202210, y_m1, test_size=0.2, random_state=2023)
X_202211_m1_train, X_202211_m1_test, y_m1_train, y_m1_test = train_test_split(X_202211, y_m1, test_size=0.2, random_state=2023)
X_202212_m1_train, X_202212_m1_test, y_m1_train, y_m1_test = train_test_split(X_202212, y_m1, test_size=0.2, random_state=2023)
X_202301_m1_train, X_202301_m1_test, y_m1_train, y_m1_test = train_test_split(X_202301, y_m1, test_size=0.2, random_state=2023)
X_202302_m1_train, X_202302_m1_test, y_m1_train, y_m1_test = train_test_split(X_202302, y_m1, test_size=0.2, random_state=2023)
X_202303_m1_train, X_202303_m1_test, y_m1_train, y_m1_test = train_test_split(X_202303, y_m1, test_size=0.2, random_state=2023)
X_202304_m1_train, X_202304_m1_test, y_m1_train, y_m1_test = train_test_split(X_202304, y_m1, test_size=0.2, random_state=2023)
X_202305_m1_train, X_202305_m1_test, y_m1_train, y_m1_test = train_test_split(X_202305, y_m1, test_size=0.2, random_state=2023)
X_202306_m1_train, X_202306_m1_test, y_m1_train, y_m1_test = train_test_split(X_202306, y_m1, test_size=0.2, random_state=2023)
X_202307_m1_train, X_202307_m1_test, y_m1_train, y_m1_test = train_test_split(X_202307, y_m1, test_size=0.2, random_state=2023)
X_202308_m1_train, X_202308_m1_test, y_m1_train, y_m1_test = train_test_split(X_202308, y_m1, test_size=0.2, random_state=2023)
X_202309_m1_train, X_202309_m1_test, y_m1_train, y_m1_test = train_test_split(X_202309, y_m1, test_size=0.2, random_state=2023)

"""W kazdym z miesiecy te same lokalizacje trafiaja do zbioru treningowego."""

X_202210_m1_train.head()

X_202309_m1_train.head()

"""#### Zbalansowanie datasetow

Balansujemy zbiory treningowe.
"""

plot_data_dist(y_m1_train)

X_202210_m1_train, y_m1_train_bal = BalanceDataSet(X_202210_m1_train, y_m1_train).useSMOTE()
X_202211_m1_train, y_m1_train_bal = BalanceDataSet(X_202211_m1_train, y_m1_train).useSMOTE()
X_202212_m1_train, y_m1_train_bal = BalanceDataSet(X_202212_m1_train, y_m1_train).useSMOTE()
X_202301_m1_train, y_m1_train_bal = BalanceDataSet(X_202301_m1_train, y_m1_train).useSMOTE()
X_202302_m1_train, y_m1_train_bal = BalanceDataSet(X_202302_m1_train, y_m1_train).useSMOTE()
X_202303_m1_train, y_m1_train_bal = BalanceDataSet(X_202303_m1_train, y_m1_train).useSMOTE()
X_202304_m1_train, y_m1_train_bal = BalanceDataSet(X_202304_m1_train, y_m1_train).useSMOTE()
X_202305_m1_train, y_m1_train_bal = BalanceDataSet(X_202305_m1_train, y_m1_train).useSMOTE()
X_202306_m1_train, y_m1_train_bal = BalanceDataSet(X_202306_m1_train, y_m1_train).useSMOTE()
X_202307_m1_train, y_m1_train_bal = BalanceDataSet(X_202307_m1_train, y_m1_train).useSMOTE()
X_202308_m1_train, y_m1_train_bal = BalanceDataSet(X_202308_m1_train, y_m1_train).useSMOTE()
X_202309_m1_train, y_m1_train_bal = BalanceDataSet(X_202309_m1_train, y_m1_train).useSMOTE()

plot_data_dist(y_m1_train_bal)

"""#### Drzewa decyzyjne"""

list_of_models_m1 = []

for i in range(12):
  list_of_models_m1.append(tree.DecisionTreeClassifier(random_state = 2023, max_depth = 8, min_samples_leaf = 10))

list_of_X_train_m1 = [X_202301_m1_train, X_202302_m1_train, X_202303_m1_train, X_202304_m1_train, X_202305_m1_train, X_202306_m1_train, X_202307_m1_train, X_202308_m1_train, X_202309_m1_train, X_202210_m1_train, X_202211_m1_train, X_202212_m1_train]

for i in range(12):
  list_of_models_m1[i].fit(list_of_X_train_m1[i], y_m1_train_bal)

"""### Model 2 - detekcja pustynia - niepustynia

#### Podzial na zbior treningowy i testowy
"""

X_202210_m2_train, X_202210_m2_test, y_m2_train, y_m2_test = train_test_split(X_202210, y_m2, test_size=0.2, random_state=2023)
X_202211_m2_train, X_202211_m2_test, y_m2_train, y_m2_test = train_test_split(X_202211, y_m2, test_size=0.2, random_state=2023)
X_202212_m2_train, X_202212_m2_test, y_m2_train, y_m2_test = train_test_split(X_202212, y_m2, test_size=0.2, random_state=2023)
X_202301_m2_train, X_202301_m2_test, y_m2_train, y_m2_test = train_test_split(X_202301, y_m2, test_size=0.2, random_state=2023)
X_202302_m2_train, X_202302_m2_test, y_m2_train, y_m2_test = train_test_split(X_202302, y_m2, test_size=0.2, random_state=2023)
X_202303_m2_train, X_202303_m2_test, y_m2_train, y_m2_test = train_test_split(X_202303, y_m2, test_size=0.2, random_state=2023)
X_202304_m2_train, X_202304_m2_test, y_m2_train, y_m2_test = train_test_split(X_202304, y_m2, test_size=0.2, random_state=2023)
X_202305_m2_train, X_202305_m2_test, y_m2_train, y_m2_test = train_test_split(X_202305, y_m2, test_size=0.2, random_state=2023)
X_202306_m2_train, X_202306_m2_test, y_m2_train, y_m2_test = train_test_split(X_202306, y_m2, test_size=0.2, random_state=2023)
X_202307_m2_train, X_202307_m2_test, y_m2_train, y_m2_test = train_test_split(X_202307, y_m2, test_size=0.2, random_state=2023)
X_202308_m2_train, X_202308_m2_test, y_m2_train, y_m2_test = train_test_split(X_202308, y_m2, test_size=0.2, random_state=2023)
X_202309_m2_train, X_202309_m2_test, y_m2_train, y_m2_test = train_test_split(X_202309, y_m2, test_size=0.2, random_state=2023)

"""W kazdym z miesiecy te same lokalizacje trafiaja do zbioru treningowego."""

X_202210_m2_train.head()

X_202309_m2_train.head()

"""#### Zbalansowanie datasetow

Balansujemy zbiory treningowe.
"""

plot_data_dist(y_m2_train)

X_202210_m2_train, y_m2_train_bal = BalanceDataSet(X_202210_m2_train, y_m2_train).useSMOTE()
X_202211_m2_train, y_m2_train_bal = BalanceDataSet(X_202211_m2_train, y_m2_train).useSMOTE()
X_202212_m2_train, y_m2_train_bal = BalanceDataSet(X_202212_m2_train, y_m2_train).useSMOTE()
X_202301_m2_train, y_m2_train_bal = BalanceDataSet(X_202301_m2_train, y_m2_train).useSMOTE()
X_202302_m2_train, y_m2_train_bal = BalanceDataSet(X_202302_m2_train, y_m2_train).useSMOTE()
X_202303_m2_train, y_m2_train_bal = BalanceDataSet(X_202303_m2_train, y_m2_train).useSMOTE()
X_202304_m2_train, y_m2_train_bal = BalanceDataSet(X_202304_m2_train, y_m2_train).useSMOTE()
X_202305_m2_train, y_m2_train_bal = BalanceDataSet(X_202305_m2_train, y_m2_train).useSMOTE()
X_202306_m2_train, y_m2_train_bal = BalanceDataSet(X_202306_m2_train, y_m2_train).useSMOTE()
X_202307_m2_train, y_m2_train_bal = BalanceDataSet(X_202307_m2_train, y_m2_train).useSMOTE()
X_202308_m2_train, y_m2_train_bal = BalanceDataSet(X_202308_m2_train, y_m2_train).useSMOTE()
X_202309_m2_train, y_m2_train_bal = BalanceDataSet(X_202309_m2_train, y_m2_train).useSMOTE()

plot_data_dist(y_m2_train_bal)

"""#### Drzewa decyzyjne"""

list_of_models_m2 = []

for i in range(12):
  list_of_models_m2.append(tree.DecisionTreeClassifier(random_state = 2023, max_depth = 10, min_samples_leaf = 10))

list_of_X_train_m2 = [X_202301_m2_train, X_202302_m2_train, X_202303_m2_train, X_202304_m2_train, X_202305_m2_train, X_202306_m2_train, X_202307_m2_train, X_202308_m2_train, X_202309_m2_train, X_202210_m2_train, X_202211_m2_train, X_202212_m2_train]

for i in range(12):
  list_of_models_m2[i].fit(list_of_X_train_m2[i], y_m2_train_bal)

"""### Model 3 - detekcja step - niestep

#### Podzial na zbior treningowy i testowy
"""

X_202210_m3_train, X_202210_m3_test, y_m3_train, y_m3_test = train_test_split(X_202210, y_m3, test_size=0.2, random_state=2023)
X_202211_m3_train, X_202211_m3_test, y_m3_train, y_m3_test = train_test_split(X_202211, y_m3, test_size=0.2, random_state=2023)
X_202212_m3_train, X_202212_m3_test, y_m3_train, y_m3_test = train_test_split(X_202212, y_m3, test_size=0.2, random_state=2023)
X_202301_m3_train, X_202301_m3_test, y_m3_train, y_m3_test = train_test_split(X_202301, y_m3, test_size=0.2, random_state=2023)
X_202302_m3_train, X_202302_m3_test, y_m3_train, y_m3_test = train_test_split(X_202302, y_m3, test_size=0.2, random_state=2023)
X_202303_m3_train, X_202303_m3_test, y_m3_train, y_m3_test = train_test_split(X_202303, y_m3, test_size=0.2, random_state=2023)
X_202304_m3_train, X_202304_m3_test, y_m3_train, y_m3_test = train_test_split(X_202304, y_m3, test_size=0.2, random_state=2023)
X_202305_m3_train, X_202305_m3_test, y_m3_train, y_m3_test = train_test_split(X_202305, y_m3, test_size=0.2, random_state=2023)
X_202306_m3_train, X_202306_m3_test, y_m3_train, y_m3_test = train_test_split(X_202306, y_m3, test_size=0.2, random_state=2023)
X_202307_m3_train, X_202307_m3_test, y_m3_train, y_m3_test = train_test_split(X_202307, y_m3, test_size=0.2, random_state=2023)
X_202308_m3_train, X_202308_m3_test, y_m3_train, y_m3_test = train_test_split(X_202308, y_m3, test_size=0.2, random_state=2023)
X_202309_m3_train, X_202309_m3_test, y_m3_train, y_m3_test = train_test_split(X_202309, y_m3, test_size=0.2, random_state=2023)

"""W kazdym z miesiecy te same lokalizacje trafiaja do zbioru treningowego."""

X_202210_m3_train.head()

X_202309_m3_train.head()

"""#### Zbalansowanie datasetow

Balansujemy zbiory treningowe.
"""

plot_data_dist(y_m3_train)

X_202210_m3_train, y_m3_train_bal = BalanceDataSet(X_202210_m3_train, y_m3_train).useSMOTE()
X_202211_m3_train, y_m3_train_bal = BalanceDataSet(X_202211_m3_train, y_m3_train).useSMOTE()
X_202212_m3_train, y_m3_train_bal = BalanceDataSet(X_202212_m3_train, y_m3_train).useSMOTE()
X_202301_m3_train, y_m3_train_bal = BalanceDataSet(X_202301_m3_train, y_m3_train).useSMOTE()
X_202302_m3_train, y_m3_train_bal = BalanceDataSet(X_202302_m3_train, y_m3_train).useSMOTE()
X_202303_m3_train, y_m3_train_bal = BalanceDataSet(X_202303_m3_train, y_m3_train).useSMOTE()
X_202304_m3_train, y_m3_train_bal = BalanceDataSet(X_202304_m3_train, y_m3_train).useSMOTE()
X_202305_m3_train, y_m3_train_bal = BalanceDataSet(X_202305_m3_train, y_m3_train).useSMOTE()
X_202306_m3_train, y_m3_train_bal = BalanceDataSet(X_202306_m3_train, y_m3_train).useSMOTE()
X_202307_m3_train, y_m3_train_bal = BalanceDataSet(X_202307_m3_train, y_m3_train).useSMOTE()
X_202308_m3_train, y_m3_train_bal = BalanceDataSet(X_202308_m3_train, y_m3_train).useSMOTE()
X_202309_m3_train, y_m3_train_bal = BalanceDataSet(X_202309_m3_train, y_m3_train).useSMOTE()

plot_data_dist(y_m3_train_bal)

"""#### Drzewa decyzyjne"""

list_of_models_m3 = []

for i in range(12):
  list_of_models_m3.append(tree.DecisionTreeClassifier(random_state = 2023, max_depth = 10, min_samples_leaf = 10))

list_of_X_train_m3 = [X_202301_m3_train, X_202302_m3_train, X_202303_m3_train, X_202304_m3_train, X_202305_m3_train, X_202306_m3_train, X_202307_m3_train, X_202308_m3_train, X_202309_m3_train, X_202210_m3_train, X_202211_m3_train, X_202212_m3_train]

for i in range(12):
  list_of_models_m3[i].fit(list_of_X_train_m3[i], y_m3_train_bal)

"""### Ocena modeli"""

list_of_X_test_m1 = [X_202301_m1_test, X_202302_m1_test, X_202303_m1_test, X_202304_m1_test, X_202305_m1_test, X_202306_m1_test, X_202307_m1_test, X_202308_m1_test, X_202309_m1_test, X_202210_m1_test, X_202211_m1_test, X_202212_m1_test]

list_of_X_test_m2 = [X_202301_m2_test, X_202302_m2_test, X_202303_m2_test, X_202304_m2_test, X_202305_m2_test, X_202306_m2_test, X_202307_m2_test, X_202308_m2_test, X_202309_m2_test, X_202210_m2_test, X_202211_m2_test, X_202212_m2_test]

list_of_X_test_m3 = [X_202301_m3_test, X_202302_m3_test, X_202303_m3_test, X_202304_m3_test, X_202305_m3_test, X_202306_m3_test, X_202307_m3_test, X_202308_m3_test, X_202309_m3_test, X_202210_m3_test, X_202211_m3_test, X_202212_m3_test]

"""#### Model 1 - detekcja pustynia - step - inne

Accuracy na zbiorze treningowym.
"""

print_accuracy_heatmap(list_of_models_m1, list_of_X_train_m1, y_m1_train_bal)

"""Accuracy na zbiorze testowym."""

print_accuracy_heatmap(list_of_models_m1, list_of_X_test_m1, y_m1_test)

"""#### Model 2 - detekcja pustynia - niepustynia

Accuracy na zbiorze treningowym.
"""

print_accuracy_heatmap(list_of_models_m2, list_of_X_train_m2, y_m2_train_bal)

"""Accuracy na zbiorze testowym."""

print_accuracy_heatmap(list_of_models_m2, list_of_X_test_m2, y_m2_test)

"""Recall na zbiorze testowym."""

print_recall_heatmap(list_of_models_m2, list_of_X_test_m2, y_m2_test)

"""Precision na zbiorze testowym."""

print_precision_heatmap(list_of_models_m2, list_of_X_test_m2, y_m2_test)

"""F1 na zbiorze testowym."""

print_f1_heatmap(list_of_models_m2, list_of_X_test_m2, y_m2_test)

"""#### Model 3 - detekcja step - niestep

Accuracy na zbiorze treningowym.
"""

print_accuracy_heatmap(list_of_models_m3, list_of_X_train_m3, y_m3_train_bal)

"""Accuracy na zbiorze testowym."""

print_accuracy_heatmap(list_of_models_m3, list_of_X_test_m3, y_m3_test)

"""Recall na zbiorze testowym."""

print_recall_heatmap(list_of_models_m3, list_of_X_test_m3, y_m3_test)

"""Precision na zbiorze testowym."""

print_precision_heatmap(list_of_models_m3, list_of_X_test_m3, y_m3_test)

"""F1 na zbiorze testowym."""

print_f1_heatmap(list_of_models_m3, list_of_X_test_m3, y_m3_test)

"""## Przedstawienie wynikow na mapach"""

display(IFrame("https://www.google.com/maps/embed?pb=!1m14!1m12!1m3!1d13982681.959428234!2d-98.66341902257437!3d38.39997874427714!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!5e1!3m2!1spl!2spl!4v1703000232420!5m2!1spl!2spl", '800px', '500px'))

"""### Dane z 07.2023

Do klasyfikacji wykorzytamy modele zbudowane na danych lipcowych.
"""

spark_month_202307 = load_single_month(spark,year=2023, month=7)

df_month_202307 = spark_month_202307.toPandas()

df_month_202307['Model_1'] = list_of_models_m1[6].predict(df_month_202307.loc[:,'Rainf':'SoilM_100_200cm'])

df_month_202307['Model_2'] = list_of_models_m2[6].predict(df_month_202307.loc[:,'Rainf':'SoilM_100_200cm'])

df_month_202307['Model_3'] = list_of_models_m3[6].predict(df_month_202307.loc[:,'Rainf':'SoilM_100_200cm'])

colormap_cluster = get_colormap([0, max(df_month_202307.Model_1.values)], ['yellow', 'darkgreen'])
plot_1 = plot_map(df=df_month_202307, parameter_name='Model_1', colormap=colormap_cluster, title="Detekcja pustynia (1) - step(2) - inne(3)", alpha=1)
output_notebook()
show(plot_1)

colormap_cluster = get_colormap([0, max(df_month_202307.Model_2.values)], ['darkgreen', 'yellow'])
plot_2 = plot_map(df=df_month_202307, parameter_name='Model_2', colormap=colormap_cluster, title="Detekcja pustynia (1) - niepustynia (0)", alpha=1)
output_notebook()
show(plot_2)

colormap_cluster = get_colormap([0, max(df_month_202307.Model_3.values)], ['darkgreen', 'yellow'])
plot_3 = plot_map(df=df_month_202307, parameter_name='Model_3', colormap=colormap_cluster, title="detekcja step (1) - niestep (0)", alpha=1)
output_notebook()
show(plot_3)

"""### Dane z 08.2023

Do klasyfikacji wykorzytamy modele zbudowane na danych grudniowych.
"""

spark_month_202308 = load_single_month(spark,year=2023, month=8)

df_month_202308 = spark_month_202308.toPandas()

df_month_202308['Model_1'] = list_of_models_m1[11].predict(df_month_202308.loc[:,'Rainf':'SoilM_100_200cm'])

df_month_202308['Model_2'] = list_of_models_m2[11].predict(df_month_202308.loc[:,'Rainf':'SoilM_100_200cm'])

df_month_202308['Model_3'] = list_of_models_m3[11].predict(df_month_202308.loc[:,'Rainf':'SoilM_100_200cm'])

colormap_cluster = get_colormap([0, max(df_month_202308.Model_1.values)], ['yellow', 'darkgreen'])
plot_4 = plot_map(df=df_month_202308, parameter_name='Model_1', colormap=colormap_cluster, title="Detekcja pustynia (1) - step(2) - inne(3)", alpha=1)
output_notebook()
show(plot_4)

colormap_cluster = get_colormap([0, max(df_month_202308.Model_2.values)], ['darkgreen', 'yellow'])
plot_5 = plot_map(df=df_month_202308, parameter_name='Model_2', colormap=colormap_cluster, title="Detekcja pustynia (1) - niepustynia (0)", alpha=1)
output_notebook()
show(plot_5)

colormap_cluster = get_colormap([0, max(df_month_202308.Model_3.values)], ['darkgreen', 'yellow'])
plot_6 = plot_map(df=df_month_202308, parameter_name='Model_3', colormap=colormap_cluster, title="detekcja step (1) - niestep (0)", alpha=1)
output_notebook()
show(plot_6)

"""### Dane z 07.1979

Do klasyfikacji wykorzytamy modele zbudowane na danych lipcowych.
"""

spark_month_197907 = load_single_month(spark,year=1979, month=7)

df_month_197907 = spark_month_197907.toPandas()

df_month_197907['Model_1'] = list_of_models_m1[6].predict(df_month_197907.loc[:,'Rainf':'SoilM_100_200cm'])

df_month_197907['Model_2'] = list_of_models_m2[6].predict(df_month_197907.loc[:,'Rainf':'SoilM_100_200cm'])

df_month_197907['Model_3'] = list_of_models_m3[6].predict(df_month_197907.loc[:,'Rainf':'SoilM_100_200cm'])

colormap_cluster = get_colormap([0, max(df_month_197907.Model_1.values)], ['yellow', 'darkgreen'])
plot_4 = plot_map(df=df_month_197907, parameter_name='Model_1', colormap=colormap_cluster, title="Detekcja pustynia (1) - step(2) - inne(3)", alpha=1)
output_notebook()
show(plot_4)

colormap_cluster = get_colormap([0, max(df_month_197907.Model_2.values)], ['darkgreen', 'yellow'])
plot_5 = plot_map(df=df_month_197907, parameter_name='Model_2', colormap=colormap_cluster, title="Detekcja pustynia (1) - niepustynia (0)", alpha=1)
output_notebook()
show(plot_5)

colormap_cluster = get_colormap([0, max(df_month_197907.Model_3.values)], ['darkgreen', 'yellow'])
plot_6 = plot_map(df=df_month_197907, parameter_name='Model_3', colormap=colormap_cluster, title="detekcja step (1) - niestep (0)", alpha=1)
output_notebook()
show(plot_6)

"""### Klsyfikacja pustyn dla kazdego miesiaca

Styczen
"""

spark_month_202301 = load_single_month(spark,year=2023, month=1)
df_month_202301 = spark_month_202301.toPandas()
plot_m2_month(df_month_202301,1)

"""Luty"""

spark_month_202302 = load_single_month(spark,year=2023, month=2)
df_month_202302 = spark_month_202302.toPandas()
plot_m2_month(df_month_202302,2)

"""Marzec"""

spark_month_202303 = load_single_month(spark,year=2023, month=3)
df_month_202303 = spark_month_202303.toPandas()
plot_m2_month(df_month_202303,3)

"""Kwiecien"""

spark_month_202304 = load_single_month(spark,year=2023, month=4)
df_month_202304 = spark_month_202304.toPandas()
plot_m2_month(df_month_202304,4)

"""Maj"""

spark_month_202305 = load_single_month(spark,year=2023, month=5)
df_month_202305 = spark_month_202305.toPandas()
plot_m2_month(df_month_202305,5)

"""Czerwiec"""

spark_month_202306 = load_single_month(spark,year=2023, month=6)
df_month_202306 = spark_month_202306.toPandas()
plot_m2_month(df_month_202306,6)

"""Lipiec"""

plot_m2_month(df_month_202307,7)

"""Sierpien"""

plot_m2_month(df_month_202308,8)

"""Wrzesien"""

spark_month_202309 = load_single_month(spark,year=2023, month=9)
df_month_202309 = spark_month_202309.toPandas()
plot_m2_month(df_month_202309,9)

"""Pazdziernik"""

spark_month_202210 = load_single_month(spark,year=2022, month=10)
df_month_202210 = spark_month_202210.toPandas()
plot_m2_month(df_month_202210,10)

"""Listopad"""

spark_month_202211 = load_single_month(spark,year=2022, month=11)
df_month_202211 = spark_month_202211.toPandas()
plot_m2_month(df_month_202211,11)

"""Grudzien"""

spark_month_202212 = load_single_month(spark,year=2022, month=12)
df_month_202212 = spark_month_202212.toPandas()
plot_m2_month(df_month_202212,12)

"""## Budowa modeli z selekcją cech na danych z jednego miesiaca

**Cel**:

Celem jest zbudowanie modeli do klasyfikacji czy wskazany punkt lokalizacyjny ze zbioru danych NASA jest: pustynia, stepem lub innym obszarem. Zbudowane zostana trzy modele:

* Model 1 - detekcja pustynia - step - inne
* Model 2 - detekcja pustynia - niepustynia
* Model 3 - detekcja step - niestep


**Proba danych**:

Dane wykorzystane do modelowania zostały stworzone po przez połączenie dwoch zbiorów danych:

* 500 lokalizacji *lon* i *lat* z określoną flagą 0, 1 w kolumnach *pustynia* lub *step* (reczna adnotacja)
* danych NASA z *stycznia 2023*

**Metoda**:

Do modelowania uzyto metody drzew decyzyjnych.

### Przygotowanie danych
"""

data=preprocessed_loader(spark,'/content/drive/MyDrive/BigMess/NASA/NASA_anotated_preprocessed.csv')

df = (
    data
    .drop('SoilT_40_100cm') #remove after fixing null values there
    .toPandas()
)
df.head()

"""Dodanie kolumny *klasa* z nastepujacym oslownikowaniem:

* **1** - pustynia
* **2** - step
* **3** - inne
"""

df['klasa'] = np.where(
                        df['pustynia'] == 1, 1, np.where(
                        df['step'] == 1,2,3)
                        )

"""Liczebnosci poszczegolnych klas w probce."""

{1:df['pustynia'].sum(), 2:df['step'].sum() , 3:df['pustynia'].count()- df['pustynia'].sum()- df['step'].sum()}

print(df.shape)

df.head(5)

"""Sprawdzenie, czy w danych występują braki - nie."""

df.isnull().sum()

"""#### Wydzielenie zbiorow danych

Zmienne kandydatki:

* **GVEG** - wskaznik roslinnosci
* **Rainf** - wskaznik opadow deszczu
* **Evap** - wskaznik calkowitej ewapotranspiracji
* **AvgSurfT** - wskaznik sredniej temperatury powierzchni ziemi
* **Albedo** - wskaznik albedo
* **SoilT_40_100cm** - wskaznik temperatury gleby w warstwie o glebokosci od 40 do 100 cm
* **PotEvap** - wskaznik potencjalnej ewapotranspiracji
* **RootMoist** - wilgotnosć gleby w strefie korzeniowej (parowanie, ktore mialoby miejsce, gdyby dostepne bylo wystarczajace zrodlo wody)
* **SoilM_100_200cm** - wilgotnosc gleby w warstwie o glebokosci od 100 do 200 cm
"""

X = df.loc[:,'Rainf':'SoilM_100_200cm']
y_m1 = df['klasa']
y_m2 = df['pustynia']
y_m3 = df['step']

"""### Model 1 - detekcja pustynia - step - inne

#### Podzial na zbior treningowy i testowy
"""

X_m1_train, X_m1_test, y_m1_train, y_m1_test = train_test_split(X, y_m1, test_size=0.2, random_state=2023)

"""#### Analiza jednoczynnikowa

Obliczmy zysk informacji.
"""

information_gain(X_m1_train, y_m1_train)

"""#### Analiza wieloczynnikowa

Obliczmy korelacje zmiennych.
"""

correlations(X_m1_train)

"""Usuniecie zmiennej *SoilT_10_40cm*. Cechuja sie ona mała wartoscia zysku informacji. (Accurancy z tymi zmiennymi 77% dla zbioru testowego.)"""

X_m1_train = X_m1_train.drop('SoilT_10_40cm',axis=1)
X_m1_test = X_m1_test.drop('SoilT_10_40cm',axis=1)

"""#### Zbalansowanie datasetu"""

plot_data_dist(y_m1_train)

X_m1_train_bal, y_m1_train_bal = BalanceDataSet(X_m1_train, y_m1_train).useSMOTE()

plot_data_dist(y_m1_train_bal)

"""#### Drzewa decyzyjne"""

tree_classifier_m1 = tree.DecisionTreeClassifier(random_state = 2023)

tree_classifier_m1.fit(X_m1_train_bal, y_m1_train_bal)

print("classifier accuracy {:.2f}%".format(tree_classifier_m1.score(X_m1_test,  y_m1_test) * 100))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.style.use("classic")
plt.figure(figsize=(30,20))
tree.plot_tree(tree_classifier_m1, max_depth=3, feature_names=X_m1_train.columns);

"""#### Ocena modelu

Na zbalansowanych danych treningowych.
"""

summary_model(tree_classifier_m1, X_m1_train_bal, y_m1_train_bal, ['1','2','3'])

print_classification_report(tree_classifier_m1, X_m1_train_bal, y_m1_train_bal)

"""Na danych testowych."""

summary_model(tree_classifier_m1, X_m1_test, y_m1_test, ['1','2','3'])

print_classification_report(tree_classifier_m1, X_m1_test, y_m1_test)

"""#### Zapisanie modelu"""

model_m1_path='/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/tree_classifier_m1'

#with open(model_m1_path, 'wb') as files:
#    pickle.dump(tree_classifier_m1, files)

"""### Model 2 - detekcja pustynia - niepustynia

#### Podzial na zbior treningowy i testowy
"""

X_m2_train, X_m2_test, y_m2_train, y_m2_test = train_test_split(X, y_m2, test_size=0.2, random_state=2023)

"""#### Analiza jednoczynnikowa

Obliczmy zysk informacji.
"""

information_gain(X_m2_train, y_m2_train)

"""#### Analiza wieloczynnikowa

Obliczmy korelacje zmiennych.
"""

correlations(X_m2_train)

"""Usuniecie zmiennej *SoilT_10_40cm*. Cechuja sie ona mała wartoscia zysku informacji. (Accurancy z tymi zmiennymi 85% dla zbioru testowego.)"""

X_m2_train = X_m2_train.drop('SoilT_10_40cm',axis=1)
X_m2_test = X_m2_test.drop('SoilT_10_40cm',axis=1)

"""#### Zbalansowanie datasetu"""

plot_data_dist(y_m2_train)

X_m2_train_bal, y_m2_train_bal = BalanceDataSet(X_m2_train, y_m2_train).useSMOTE()

plot_data_dist(y_m2_train_bal)

"""#### Drzewa decyzyjne"""

tree_classifier_m2 = tree.DecisionTreeClassifier(random_state = 2023)

tree_classifier_m2.fit(X_m2_train_bal, y_m2_train_bal)

print("classifier accuracy {:.2f}%".format(tree_classifier_m2.score(X_m2_test,  y_m2_test) * 100))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.style.use("classic")
plt.figure(figsize=(30,20))
tree.plot_tree(tree_classifier_m2, max_depth=3, feature_names=X_m2_train.columns);

"""#### Ocena modelu

Na zbalansowanych danych treningowych.
"""

summary_model(tree_classifier_m2, X_m2_train_bal, y_m2_train_bal, ['0', '1'])

print_classification_report(tree_classifier_m2, X_m2_train_bal, y_m2_train_bal)

"""Na danych testowych."""

summary_model(tree_classifier_m2, X_m2_test, y_m2_test, ['0', '1'])

print_classification_report(tree_classifier_m2, X_m2_test, y_m2_test)

"""#### Zapisanie modelu"""

model_m2_path='/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/tree_classifier_m2'

#with open(model_m2_path, 'wb') as files:
#    pickle.dump(tree_classifier_m2, files)

"""### Model 3 - detekcja step - niestep

#### Podzial na zbior treningowy i testowy
"""

X_m3_train, X_m3_test, y_m3_train, y_m3_test = train_test_split(X, y_m3, test_size=0.2, random_state=2023)

"""#### Analiza jednoczynnikowa

Obliczmy zysk informacji.
"""

information_gain(X_m3_train, y_m3_train)

"""#### Analiza wieloczynnikowa

Obliczmy korelacje zmiennych.
"""

correlations(X_m3_train)

"""Usuniecie zmiennych *AvgSurfT*, *Evap* i *PotEvap*. Cechuja sie one mała wartoscia zysku informacji. (Accurancy z tymi zmiennymi 78% dla zbioru testowego.)"""

X_m3_train = X_m3_train.drop('AvgSurfT',axis=1)
X_m3_test = X_m3_test.drop('AvgSurfT',axis=1)
X_m3_train = X_m3_train.drop('Evap',axis=1)
X_m3_test = X_m3_test.drop('Evap',axis=1)
X_m3_train = X_m3_train.drop('PotEvap',axis=1)
X_m3_test = X_m3_test.drop('PotEvap',axis=1)

"""#### Zbalansowanie datasetu"""

plot_data_dist(y_m3_train)

X_m3_train_bal, y_m3_train_bal = BalanceDataSet(X_m3_train, y_m3_train).useSMOTE()

plot_data_dist(y_m3_train_bal)

"""#### Drzewa decyzyjne"""

tree_classifier_m3 = tree.DecisionTreeClassifier(random_state = 2023)

tree_classifier_m3.fit(X_m3_train_bal, y_m3_train_bal)

print("classifier accuracy {:.2f}%".format(tree_classifier_m3.score(X_m3_test,  y_m3_test) * 100))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.style.use("classic")
plt.figure(figsize=(30,20))
tree.plot_tree(tree_classifier_m3, max_depth=3, feature_names=X_m3_train.columns);

"""#### Ocena modelu

Na zbalansowanych danych treningowych.
"""

summary_model(tree_classifier_m3, X_m3_train_bal, y_m3_train_bal, ['0', '1'])

print_classification_report(tree_classifier_m3, X_m3_train_bal, y_m3_train_bal)

"""Na danych testowych."""

summary_model(tree_classifier_m3, X_m3_test, y_m3_test, ['0', '1'])

print_classification_report(tree_classifier_m3, X_m3_test, y_m3_test)

"""#### Zapisanie modelu"""

model_m3_path='/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/tree_classifier_m3'

#with open(model_m3_path, 'wb') as files:
#    pickle.dump(tree_classifier_m3, files)

"""### Podsumowanie

**Model 1 - detekcja pustynia - step - inne** \
Zmienne:

*	Rainf              
*	Evap
* PotEvap                
*	AvgSurfT           
*	Albedo             
*	SoilM_100_200cm
*	GVEG                           
*	RootMoist          


Zwraca *1*, *2* lub *3* gdzie:
* 1 - pustynia
* 2 - step
* 3 - inne

**Model 2 - detekcja pustynia - niepustynia** \
Zmienne:

*	Rainf              
*	Evap
* PotEvap                
*	AvgSurfT           
*	Albedo             
*	SoilM_100_200cm
*	GVEG                           
*	RootMoist        


Zwraca *0* lub *1* gdzie:
* 0 - niepustynia
* 1 - pustynia

**Model 3 - detekcja step - niestep** \
Zmienne:

*	Rainf              
*	SoilT_10_40cm                           
*	Albedo             
*	SoilM_100_200cm
*	GVEG                           
*	RootMoist       


Zwraca *0* lub *1* gdzie:
* 0 - niestep
* 1 - step

Sciezki do modeli:
"""

model_m1_path='/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/tree_classifier_m1'
model_m2_path='/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/tree_classifier_m2'
model_m3_path='/content/drive/MyDrive/BigMess/NASA/Modele/Klasyfikacja/tree_classifier_m3'

"""Przykladowy odczyt modelu."""

with open(model_m1_path , 'rb') as f:
    model = pickle.load(f)