{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrMaciejKowalski/BigData2024Project/blob/Selekcja-cech-ograniczenie-zbioru-i-pobranie-danych/colabs/Nowy_szablon_wczytania_danych_(po_selekcji_cech).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wczytywanie danych w sparku"
      ],
      "metadata": {
        "id": "tM663YMlg_E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utworzenie środowiska pyspark do obliczeń:"
      ],
      "metadata": {
        "id": "2gBFVF2WoyXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "YzSDuhpnh9q6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "xrwtGNCnOMig"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "02DvjdYfON7R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import DataFrame as SparkDataFrame\n",
        "from pyspark.sql.types import IntegerType, FloatType, StringType, StructType"
      ],
      "metadata": {
        "id": "r626WrHnO4Pp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utowrzenie sesji:"
      ],
      "metadata": {
        "id": "1yEZdu1Ko4II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "pG05GbFHhmNI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Połączenie z dyskiem:"
      ],
      "metadata": {
        "id": "RJ5qVugtpApM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cDV6XvNgTdV",
        "outputId": "e9e733e5-c6a1-4f60-c82f-c60b6a9159d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytanie danych NASA znajdujących się na dysku w sparku:"
      ],
      "metadata": {
        "id": "EDZ6ImTlpHOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['lon', 'lat', 'Date', 'Rainf', 'Evap', 'AvgSurfT', 'Albedo','SoilT_10_40cm', 'GVEG', 'PotEvap', 'RootMoist', 'SoilM_100_200cm']\n",
        "\n",
        "# Utworzenie schematu określającego typ zmiennych\n",
        "schema = StructType()\n",
        "for i in columns:\n",
        "  if i == \"Date\":\n",
        "    schema = schema.add(i, IntegerType(), True)\n",
        "  else:\n",
        "    schema = schema.add(i, FloatType(), True)"
      ],
      "metadata": {
        "id": "_Goo147Lzr6B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wczytanie zbioru Nasa w sparku\n",
        "nasa = spark.read.format('csv').option(\"header\", True).schema(schema).load('/content/drive/MyDrive/BigMess/NASA/NASA.csv')"
      ],
      "metadata": {
        "id": "Anm2lmTahQJx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zanim zaczniemy pisać kwerendy należy jeszcze dodać nasz DataFrame (df) do \"przestrzeni nazw tabel\" Sparka:"
      ],
      "metadata": {
        "id": "p3qU94A9p5BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nasa.createOrReplaceTempView(\"nasa\")"
      ],
      "metadata": {
        "id": "bSPBX5fPi7Ub"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rozdzielenie kolumny \"Date\" na kolumny \"Year\" oraz \"Month\""
      ],
      "metadata": {
        "id": "OLvsomiX7RxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nasa_ym = spark.sql(\"\"\"\n",
        "          SELECT\n",
        "          CAST(SUBSTRING(CAST(Date AS STRING), 1, 4) AS INT) AS Year,\n",
        "          CAST(SUBSTRING(CAST(Date AS STRING), 5, 2) AS INT) AS Month,\n",
        "          n.*\n",
        "          FROM nasa n\n",
        "          \"\"\")\n",
        "nasa_ym = nasa_ym.drop(\"Date\")"
      ],
      "metadata": {
        "id": "PUp1C5Qq3k-5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nasa_ym.createOrReplaceTempView(\"nasa_ym\")"
      ],
      "metadata": {
        "id": "dWbFnmv97rvu"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}