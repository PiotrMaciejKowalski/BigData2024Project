{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMNh4F2hl4Ad9ohxWjBy37",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrMaciejKowalski/BigData2024Project/blob/refactoring-sprint2/colabs/BigMess_preprocessing_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cel notatnika\n",
        "\n",
        "Notatnik prezentuje ogólny setup preprocessingu w projekcie `big mess` - można go używać do prezentowania wynik wstępnej analizy, ale oprócz niego powstaje drugi notatnik `BigMess-preprocessing-full`, który ma stanowić odpowiednik przetwarzania całości.\n",
        "\n",
        "Notatnik jednak w ogólności niedotyczy prac nad zadaniem sprintu 2 o budowie modelu rozpoznającego pustynie\n",
        "\n",
        "# Setup środowiska\n",
        "\n",
        "Najpierw instalacja sparka"
      ],
      "metadata": {
        "id": "lrWZed6gEinO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw6tiFsNESrW"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install poetry # do budowania paczek"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalej importujemy nasz projekt do zasobów. Będzie go widać po lewej stronie w zakładce files\n"
      ],
      "metadata": {
        "id": "6hBJc8izFmIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PiotrMaciejKowalski/BigData2024Project.git"
      ],
      "metadata": {
        "id": "hue453vyF3fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Budujemy lokalna paczkę z kodem"
      ],
      "metadata": {
        "id": "kUdzJFnxF_Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd BigData2024Project/\n",
        "!poetry install\n",
        "!pip install -e .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "XJFH_7YyGDHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart runtime\n",
        "\n",
        "Potem występuje trudny krok teraz (prawdopodobnie) trzeba **ZRESTARTOWAĆ RUNTIME**. Wybieramy sekcje Runtime i Restart runtime\n",
        "\n",
        "## Start sparka\n"
      ],
      "metadata": {
        "id": "JhFP-_jEGJhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "_qO1BEp3GTFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "Na początek importy, stworzenie sesji pythona i podpięcie zasobów"
      ],
      "metadata": {
        "id": "XkM81-KDGU8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from bigmess.loader import default_loader"
      ],
      "metadata": {
        "id": "xE6XV6OvGalq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "pIWS7soaGvkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WyPcyu0bGwT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nasa = default_loader(spark)"
      ],
      "metadata": {
        "id": "0tMnV7OjLfbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}