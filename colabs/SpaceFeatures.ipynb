{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJdUA-yPkXst",
        "outputId": "eb9afaeb-e3a0-4b10-b6fd-fc384ebc44ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=e1ba275013f3d45ac4fe08380f54557b6240fd91cbb1dfc1fbd9b74edb062951\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from google.colab import drive\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import size\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType, FloatType, StringType, StructType\n",
        "from pyspark.sql import Window\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QbLADY13kjBr"
      },
      "outputs": [],
      "source": [
        "# tworzenie sesji w Sparku\n",
        "spark = SparkSession.builder.appName('SparkWindows').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C4-zbxnhkrVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a4a7d5-2fe5-48d1-ebef-5986dda9891b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "+---------+-------+------+-----+---------+---------+---------+-------------+----------+---------+----------+---------------+\n",
            "|      lon|    lat|  Date|Rainf|     Evap| AvgSurfT|   Albedo|SoilT_10_40cm|      GVEG|  PotEvap| RootMoist|SoilM_100_200cm|\n",
            "+---------+-------+------+-----+---------+---------+---------+-------------+----------+---------+----------+---------------+\n",
            "|-112.0625|25.0625|200001|  0.0|   4.3807| 288.0707| 41.47715|    289.00714|0.19712792|139.13737|  243.2525|      108.76931|\n",
            "|-111.9375|25.0625|200001|  0.0|4.6673994|287.39276|41.509407|     288.8017|0.19860405|162.25638| 220.77466|       90.67495|\n",
            "|-111.8125|25.0625|200001|  0.0|5.8487973| 287.6554|41.505375|    289.55984|0.17118543|121.55404| 103.95005|      161.94794|\n",
            "|-111.6875|25.0625|200001|  0.0|6.4366016| 287.5386|41.501343|    289.61142|0.17118543|127.63407|106.032845|      163.44402|\n",
            "|-111.5625|25.0625|200001|  0.0|3.4506986| 287.2394|41.509407|     289.2371| 0.1429876|179.37668| 161.43001|      187.07886|\n",
            "+---------+-------+------+-----+---------+---------+---------+-------------+----------+---------+----------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# wczytanie danych z google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "columns = ['lon', 'lat', 'Date', 'Rainf', 'Evap', 'AvgSurfT', 'Albedo','SoilT_10_40cm', 'GVEG', 'PotEvap', 'RootMoist', 'SoilM_100_200cm']\n",
        "\n",
        "# Utworzenie schematu okreslajacego typ zmiennych\n",
        "schema = StructType()\n",
        "for i in columns:\n",
        "  if i == \"Date\":\n",
        "    schema = schema.add(i, IntegerType(), True)\n",
        "  else:\n",
        "    schema = schema.add(i, FloatType(), True)\n",
        "\n",
        "nasa = spark.read.format('csv').option(\"header\", True).schema(schema).load('/content/drive/MyDrive/BigMess/NASA/NASA.csv')\n",
        "nasa.createOrReplaceTempView(\"nasa\")\n",
        "nasa.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5wvmZ5_isGZe"
      },
      "outputs": [],
      "source": [
        "nasa_ym = spark.sql(\"\"\"\n",
        "          SELECT\n",
        "          CAST(SUBSTRING(CAST(Date AS STRING), 1, 4) AS INT) AS Year,\n",
        "          CAST(SUBSTRING(CAST(Date AS STRING), 5, 2) AS INT) AS Month,\n",
        "          n.*\n",
        "          FROM nasa n\n",
        "          \"\"\")\n",
        "nasa_ym = nasa_ym.drop(\"Date\")\n",
        "\n",
        "nasa_ym.createOrReplaceTempView(\"nasa_ym\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wybieramy dane z lipca 2k23\n",
        "SparkDataFrame_2023_7 = spark.sql(\"\"\"\n",
        "                        SELECT\n",
        "                        *\n",
        "                        FROM nasa_ym WHERE (Year == 2023) and (Month == 7)\n",
        "                        order by lon, lat, Year, Month\n",
        "                        \"\"\")\n",
        "SparkDataFrame_2023_7.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weBh85IsRC2G",
        "outputId": "800a89a7-92b9-4644-d93a-51a4bfb15ce4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+---------+-------+--------+--------+--------+--------+-------------+---------+--------+---------+---------------+\n",
            "|Year|Month|      lon|    lat|   Rainf|    Evap|AvgSurfT|  Albedo|SoilT_10_40cm|     GVEG| PotEvap|RootMoist|SoilM_100_200cm|\n",
            "+----+-----+---------+-------+--------+--------+--------+--------+-------------+---------+--------+---------+---------------+\n",
            "|2023|    7|-124.9375|48.8125|52.83326|35.82973|286.1314|19.78629|      282.349| 0.853646|179.8188| 453.1293|       229.7097|\n",
            "|2023|    7|-124.9375|48.9375|38.92641| 46.2698|288.2968|19.52688|     284.2224| 0.853646|224.1511| 416.8515|       212.1873|\n",
            "|2023|    7|-124.9375|49.0625|28.72708|43.29089|287.6732|19.38844|     283.7652|0.8625529|249.8481| 404.1081|       209.3529|\n",
            "|2023|    7|-124.9375|49.1875| 22.0683| 45.7691|288.7706|19.38441|     284.7356|0.8625529|265.3578| 391.5402|       204.5058|\n",
            "|2023|    7|-124.9375|49.3125| 19.8993|54.68368|291.3871|19.38038|     286.9337|0.8549436|263.7617| 371.7049|       194.5398|\n",
            "+----+-----+---------+-------+--------+--------+--------+--------+-------------+---------+--------+---------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tworze sztuczna tabele ktora posluzy mi do wstepnego sprawdzenia funkcji w wersji pandasowej\n",
        "from pyspark.sql.types import StructType, StructField, FloatType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"lon\", FloatType(), True),\n",
        "    StructField(\"lat\", FloatType(), True),\n",
        "    StructField(\"GVEG\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [(-112.0625, 25.0625, 45),\n",
        "        (-104.4165, 32.4484, 20),\n",
        "        (-112.0111, 25.0995, 30),\n",
        "        (-104.3406, 32.5318, 40),\n",
        "        (-111.9428, 25.0280, 50)]\n",
        "\n",
        "dataF = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Wyświetlenie zawartości tabeli\n",
        "dataF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw8ZkcEvRyvg",
        "outputId": "bc3596f9-3288-44f2-dbe2-28063de33ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+----+\n",
            "|      lon|    lat|GVEG|\n",
            "+---------+-------+----+\n",
            "|-112.0625|25.0625|  45|\n",
            "|-104.4165|32.4484|  20|\n",
            "|-112.0111|25.0995|  30|\n",
            "|-104.3406|32.5318|  40|\n",
            "|-111.9428| 25.028|  50|\n",
            "+---------+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from pyspark.sql.functions import col, lit, when\n",
        "\n",
        "# funkcja, która dzialaa tylko, że konwertuje tabele Spark na Pandas\n",
        "def space_features(dataframe):\n",
        "    # Przeksztalcenie DataFrame na GeoDataFrame\n",
        "    geometry = [Point(lon, lat) for lon, lat in zip(dataframe.select(\"lon\").rdd.flatMap(lambda x: x).collect(), dataframe.select(\"lat\").rdd.flatMap(lambda x: x).collect())]\n",
        "    gdf = gpd.GeoDataFrame(dataframe.toPandas(), geometry=geometry)\n",
        "\n",
        "    # Funkcja do obliczania sredniej GVEG w odleglosci 50 km\n",
        "    def average_gveg_within_radius(center_point, radius=50):\n",
        "        circle = center_point.buffer(radius / 111.32)  # Przyblizona konwersja stopni na kilometr (1 stopien to okolo 111.32 km)\n",
        "        points_within_circle = gdf[gdf.geometry.within(circle)]\n",
        "        if len(points_within_circle) > 0:\n",
        "            return points_within_circle[\"GVEG\"].mean()\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Dodanie kolumny avgGVEG\n",
        "    df_with_avg_gveg = dataframe.withColumn(\"avgGVEG\", lit(None).cast(\"float\"))  # Inicjalizacja kolumny avgGVEG\n",
        "\n",
        "    # Obliczenia dla kazdego wiersza\n",
        "    for row in dataframe.collect():\n",
        "        lon, lat, _ = row\n",
        "        center_point = Point(lon, lat)\n",
        "        avg_gveg = average_gveg_within_radius(center_point)\n",
        "        df_with_avg_gveg = df_with_avg_gveg.withColumn(\"avgGVEG\", when((col(\"lon\") == lon) & (col(\"lat\") == lat), avg_gveg).otherwise(col(\"avgGVEG\")))\n",
        "    return df_with_avg_gveg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_HibGoMIVHcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wynik_sztucznej_tabeli = space_features(dataF)\n",
        "wynik_sztucznej_tabeli.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvEKOK7qWJ-d",
        "outputId": "13290d31-b4fe-49c8-b0a3-dfa613022ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+----+------------------+\n",
            "|      lon|    lat|GVEG|           avgGVEG|\n",
            "+---------+-------+----+------------------+\n",
            "|-112.0625|25.0625|  45|41.666666666666664|\n",
            "|-104.4165|32.4484|  20|              30.0|\n",
            "|-112.0111|25.0995|  30|41.666666666666664|\n",
            "|-104.3406|32.5318|  40|              30.0|\n",
            "|-111.9428| 25.028|  50|41.666666666666664|\n",
            "+---------+-------+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# teraz zrobimy test na pierwszy 500 wierszach naszej prawdziwej tabeli, ale dla przyspieszenia procesu ograniczymy sie tylko do trzech kolumn\n",
        "selected_columns_2023_7 = SparkDataFrame_2023_7.select(col(\"lon\"), col(\"lat\"), col(\"GVEG\"))\n",
        "first_500 = selected_columns_2023_7.limit(500)"
      ],
      "metadata": {
        "id": "KaqSZx9jlSix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wynik_first_500 = space_features(first_500)\n",
        "wynik_first_500.show(5)"
      ],
      "metadata": {
        "id": "zQGCkuPjk7YK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbc2290-c96a-4721-e68c-b1e521301254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------+------------------+\n",
            "|      lon|    lat|     GVEG|           avgGVEG|\n",
            "+---------+-------+---------+------------------+\n",
            "|-124.9375|48.8125| 0.853646| 0.860625684261322|\n",
            "|-124.9375|48.9375| 0.853646|0.8644187450408936|\n",
            "|-124.9375|49.0625|0.8625529|0.8666259050369263|\n",
            "|-124.9375|49.1875|0.8625529|0.8674166798591614|\n",
            "|-124.9375|49.3125|0.8549436|0.8665063381195068|\n",
            "+---------+-------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tutaj to zadzialalo ale jezeli zastosujemy te funkcje do calego zbioru z lipca 2k23 to dostajemy bledy o przekroczeniu limitow."
      ],
      "metadata": {
        "id": "qrjJnciub-ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ponizej proby na oknach w Sparku"
      ],
      "metadata": {
        "id": "aqkPg2DIcPNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# funkcja do liczenia odleglosci\n",
        "haversine_udf = udf(lambda lon1, lat1, lon2, lat2: haversine_distance(lon1, lat1, lon2, lat2), DoubleType())"
      ],
      "metadata": {
        "id": "xCrYA35R4YD8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tutaj mamy funkcje ktora teoretycznie moglaby robic to czego potrzebujemy, a pod nia po kolei test jej kolejnych krokow\n",
        "\n",
        "def calculate_average_within_distance(km: int):\n",
        "    # Filtruj wiersze, aby uniknąć kombinacji tych samych współrzędnych geograficznych + wybieramy ten sam moment w czasie\n",
        "    filtered_nasa = nasa.crossJoin(nasa.withColumnRenamed(\"lon\", \"lon2\").withColumnRenamed(\"lat\", \"lat2\"))\n",
        "    # obliczamy odleglosc dla kazdej pary wspolrzednych\n",
        "    filtered_nasa = filtered_nasa.withColumn('distance_km', haversine_udf('lon', 'lat', 'lon2', 'lat2'))\n",
        "\n",
        "    # wybieramy te pary dla ktorych odleglosc jest mniejsza od podanej w parametrze funkcji\n",
        "    filtered_nasa = filtered_nasa.filter((col(\"distance\") > 0) & (col(\"distance\") <= km))\n",
        "\n",
        "    # towrzymy okno\n",
        "    window_spec = Window().partitionBy(\"lon\", \"lat\").orderBy(\"distance\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "    # liczymy srednia z kolumny GVEG z punktow w danych na podanym obszarze\n",
        "    result_nasa = filtered_nasa.withColumn(\"avgGVEG\", F.avg(\"gveg\").over(window_spec))\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "TtteY9_hhTso"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_nasa = SparkDataFrame_2023_7.crossJoin(SparkDataFrame_2023_7.withColumnRenamed(\"lon\", \"lon2\").withColumnRenamed(\"lat\", \"lat2\"))"
      ],
      "metadata": {
        "id": "4hSF-om3iGk3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming filtered_nasa is your PySpark DataFrame\n",
        "filtered_nasa = filtered_nasa.withColumn('distance', haversine_udf('lon', 'lat', 'lon2', 'lat2'))"
      ],
      "metadata": {
        "id": "-JN5lUov0MdY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wybieramy te pary dla ktorych odleglosc jest mniejsza od podanej w parametrze funkcji\n",
        "filtered_nasa = filtered_nasa.filter((col(\"distance\") > 0) & (col(\"distance\") <= 50))"
      ],
      "metadata": {
        "id": "JERwdsCVP4Hm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzymy okno dla partycji zgrupowanych według 'lon' i 'lat' oraz sortujemy po 'distance'\n",
        "window_spec = Window().partitionBy(\"lon\", \"lat\").orderBy(\"distance\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)"
      ],
      "metadata": {
        "id": "C0LNTiAz-Mek"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzymy kolumne 'avgGVEG' z wartosciami srednimi dla 'gveg' w obrebie 50 jednostek odleglosci\n",
        "result_nasa = filtered_nasa.withColumn(\"avgGVEG\", F.avg(\"gveg\").over(window_spec))\n",
        "\n",
        "# trzeba usunac kolumny ktore zostaly zduplikowane przy corssjoinie tylko nie wiem jak bo jesli robie .drop('zduplikowana_kolumna') to pisze ze nie moze bo nie wie ktora. odniesienie sie do numeracji ze np. usun kolumne 5\n",
        "# powoduje ten sam blad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "WjFROx2q-P3u",
        "outputId": "4207e89a-8f8a-4f78-aa73-afec84e289bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[AMBIGUOUS_REFERENCE] Reference `gveg` is ambiguous, could be: [`nasa_ym`.`gveg`, `nasa_ym`.`gveg`].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c007a0639a86>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tworzymy kolumnę 'avgGVEG' z wartościami średnimi dla 'gveg' w obrębie 50 jednostek odległości\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_nasa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_nasa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avgGVEG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gveg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5168\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5169\u001b[0m             )\n\u001b[0;32m-> 5170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `gveg` is ambiguous, could be: [`nasa_ym`.`gveg`, `nasa_ym`.`gveg`]."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}